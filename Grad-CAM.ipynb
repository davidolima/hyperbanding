{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe35035-fba2-41e4-9403-4798b75b08ff",
   "metadata": {},
   "source": [
    "# GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57b7b6-8b64-4e67-934e-2f853508c57d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43839f7-100c-4d72-97d8-1ac2a50db005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# GradCAM\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# My Imports\n",
    "sys.path.append('../pytorch/')\n",
    "from configs import Inputs\n",
    "from train import get_classification_model\n",
    "#from utils.data import RadiographSexDataset\n",
    "from utils.data import FullRadiographSexDataset\n",
    "from utils.augmentations import get_transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1f3cc-ac8c-4998-9773-d8d8ca490963",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4b81e-4902-40b2-bbd6-896494978bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img, means, stdvs):\n",
    "    means = torch.tensor(means)\n",
    "    stdvs = torch.tensor(stdvs)\n",
    "    return means + stdvs*img.squeeze().permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6d2f6-413b-44ba-95bf-0a5eb391abd0",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa33d3-4dde-4fb4-a564-7f624aa7e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stdvs = [0.229, 0.224, 0.225]\n",
    "transform = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(means, stdvs)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1ae2a-b0ab-473e-956b-9000582b8e57",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469dbdd-ce85-45de-a12b-4c5cf6356603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and weights\n",
    "inputs = Inputs(selected_model='efficientnet-b0')\n",
    "model = get_classification_model(inputs.model_name, 2)\n",
    "checkpoint = torch.load('/home/bernardo/github/sex-age-estimation/backup-bia/patch-1/pesos/checkpoint-efficientnet-b0-fold-2-max-acc.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for i in range(1,31):\n",
    "    filepath = f'/home/bernardo/datasets/pan-radiographs/splits/{i:02d}.txt'\n",
    "    with open(filepath) as f:\n",
    "        for line in f: #ler cada linha do txt\n",
    "            fname = line.strip().split('/')[2] #retirar o \\n\n",
    "            sex = fname.split('-')[10]\n",
    "            age = fname.split('-')[-2][1:]\n",
    "            months = fname.split('-')[-1][1:3] #home/bernardo/datasets/pan-radiographs/1st-set\n",
    "\n",
    "            if fname.split('-')[0] == 'pan': #separar os arquivos pan e panreport\n",
    "                fpath = os.path.join(f'/home/bernardo/datasets/pan-radiographs/1st-set/images/{fname}')\n",
    "           \n",
    "            else:\n",
    "                fpath = os.path.join(f'/home/bernardo/datasets/pan-radiographs/2nd-set/images/{fname}')\n",
    "            im = Image.open(fpath)\n",
    "\n",
    "val_dataset = FullRadiographSexDataset(\n",
    "    root_dir=inputs.DATASET_DIR,\n",
    "    fold_nums=inputs.val_folds,\n",
    "    transforms=get_transforms(inputs, subset='val') #aqui não tá indo de primeira. coloquei primeiro transforms = transform da celula anterior, mas só funcionou o gradcam com get_transforms\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a620ac2-ee50-488d-9e04-dd2fa143eaa1",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a627a7e-0cd5-43a3-8254-6305297b630b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = None # uses highest score category\n",
    "\n",
    "# target_layers = [model.layer4[-1]]# this is the last layer for resnet\n",
    "#target_layers = [model.classifier[1]] # this is the last layer for efficientnet b0\n",
    "\n",
    "target_layers = [model.features[-1]] ## this is the last layer for efficientnet b1 e b0\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "attention = torch.zeros((224,224))  #(224,224) para a b0 e (240,240) para a b1\n",
    "model = model.cuda()\n",
    "for idx, (img, label) in tqdm(enumerate(val_dataloader)):\n",
    "    img, label = img.cuda(), label.cuda()\n",
    "    preds = model(img)\n",
    "    prediction = torch.argmax(preds).item()\n",
    "    ground_truth = label.item()\n",
    "    \n",
    "    grayscale_cam = cam(input_tensor=img, targets=None)\n",
    "\n",
    "     # In this example grayscale_cam has only one image in the batch:                  \n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # Average all:\n",
    "    attention += grayscale_cam/len(val_dataloader)\n",
    "\n",
    "    image = denorm(img.cpu(), inputs.MEAN, inputs.STDV).cpu().numpy()\n",
    "    #visualize = show_cam_on_image(image, attention, use_rgb=True) \n",
    "    visualize = show_cam_on_image(image, grayscale_cam, use_rgb=True) #para visualizar uma imagem de cada vez\n",
    "    plt.imshow(visualize)\n",
    "    #break # para visualizar apenas 1 imagem de cada vez sem carregar o total de imagens do fold que você escolheu para visualizar (tem diferença)\n",
    "#para rodar o mapa de atenção médio tem que rodar essa celula sem o break acima, porque ele vai fazer uma varredura com todas imagen da validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d65be-ddc5-43ee-b644-a1521d8e8a48",
   "metadata": {},
   "source": [
    "## Grad CAM médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb9b14-c305-41c4-b396-8c0f2c00215d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize = show_cam_on_image(image, attention, use_rgb=True)\n",
    "plt.imshow(visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14cbe3-0945-4d7b-9386-ba52a86def1a",
   "metadata": {},
   "source": [
    "## Pontos máximos de atenção do GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92490e3e-b356-4c2b-a0a8-ca0ac7e9742b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pontos máximos de atenção sem uma imagem específica \n",
    "import numpy as np\n",
    "media = attention.squeeze().cpu().numpy()\n",
    "plt.imshow(media)\n",
    "np.where(media == media.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e19e4-60bd-4ff5-ba30-4a0aaafc3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição da rede para essa imagem\n",
    "prediction = torch.argmax(preds).item()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a32eb0-e091-44ad-870d-1fe3219db45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rótulo verdadeiro da imagem\n",
    "# Homem = 1\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4795462-6953-411a-b341-8b17a4b394e9",
   "metadata": {},
   "source": [
    "## Show GradCAM on the last image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d357bd-8084-4f88-b27a-c7561295a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_image = denorm(img[0].cpu(), means, stdvs).cpu().numpy()\n",
    "#image = denorm(img.cpu(), inputs.MEAN, inputs.STDV).cpu().numpy()\n",
    "visualize = show_cam_on_image(last_image, grayscale_cam, use_rgb=True)\n",
    "plt.imshow(visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258cffb-f2de-44da-ae94-8b563a332e5a",
   "metadata": {},
   "source": [
    "## Outras análises do GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c598849-a701-434b-aa07-241b1e50f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_attention(att):\n",
    "    if isinstance(att, torch.Tensor):\n",
    "        att = att.detach().numpy()\n",
    "\n",
    "    y_max, x_max = np.unravel_index(np.argmax(att), att.shape)\n",
    "\n",
    "    return x_max, y_max\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cbd2a-7594-44c3-b5bd-5f4c64d59819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# available_cams = [GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad]\n",
    "available_cams = [GradCAM, GradCAMPlusPlus, XGradCAM, EigenCAM]\n",
    "\n",
    "attention_maps = []\n",
    "all_max_attention_points = []\n",
    "size = len(val_dataloader) // 20 # len(val_dataloader)\n",
    "for selected_cam in available_cams:\n",
    "    print('.', end='')\n",
    "    cam = selected_cam(model=model, target_layers=target_layers, use_cuda=False)\n",
    "\n",
    "    max_attention_points = []\n",
    "    attention = torch.zeros((224, 224))\n",
    "\n",
    "    for idx, (imgs, labels) in enumerate(val_dataloader):\n",
    "        imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "        # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "        grayscale_cams = cam(input_tensor=imgs, targets=targets)\n",
    "\n",
    "        # In this example grayscale_cam has only one image in the batch:\n",
    "        for grayscale_cam in grayscale_cams:\n",
    "            # max attention points\n",
    "            max_attention_points.append(max_attention(grayscale_cam))\n",
    "\n",
    "            # average all\n",
    "            attention += grayscale_cam / size / batch_size\n",
    "\n",
    "        if idx == size - 1:\n",
    "            break\n",
    "    \n",
    "    attention_maps.append(attention)\n",
    "    all_max_attention_points.append(max_attention_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02b1c-d6bd-4db4-ae16-d7dd66b086ea",
   "metadata": {},
   "source": [
    "## Pontos de atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112013ae-da39-4c54-b964-3bd14bb0c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(attention_maps)\n",
    "fig, axes = plt.subplots(1, n, figsize=(20, n*20))\n",
    "for i, (attention, max_attention_points) in enumerate(zip(attention_maps, all_max_attention_points)):\n",
    "    Xs = [x for x, y in max_attention_points]\n",
    "    Ys = [y for x, y in max_attention_points]\n",
    "    axes[i].imshow(attention)\n",
    "    axes[i].plot(Xs, Ys, 'ro', alpha=5/len(max_attention_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af352dc-c3c9-4d9e-a237-d04cdeab26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.xlim(0, 224)\n",
    "plt.ylim(224, 0)\n",
    "\n",
    "X = [p[0] for p in max_attention_points]\n",
    "Y = [p[1] for p in max_attention_points]\n",
    "image = denorm(img.cpu(), inputs.MEAN, inputs.STDV).cpu().numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "# plt.imshow(attention.detach().numpy())\n",
    "\n",
    "for x in range(7):\n",
    "    xx = 32*x\n",
    "    plt.plot([xx, xx], [0, 224], 'g--')\n",
    "\n",
    "for y in range(7):\n",
    "    yy = 32*y\n",
    "    plt.plot([0, 224], [yy, yy], 'b--')\n",
    "\n",
    "plt.plot(X, Y, 'ro', alpha=10/size/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85271f9a-852d-49da-9397-1984b95d516a",
   "metadata": {},
   "source": [
    "## EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d06c63-b712-4c3c-9831-738bd5fb961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.features[-1]] ## this is the last layer for efficientnet b1 e b0\n",
    "cam = EigenCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "attention = torch.zeros((224,224))  #(224,224) para a b0 e (240,240) para a b1\n",
    "model = model.cuda()\n",
    "for idx, (img, label) in tqdm(enumerate(val_dataloader)):\n",
    "    img, label = img.cuda(), label.cuda()\n",
    "    preds = model(img)\n",
    "    prediction = torch.argmax(preds).item()\n",
    "    ground_truth = label.item()\n",
    "    \n",
    "    grayscale_cam = cam(input_tensor=img, targets=None)\n",
    "\n",
    "     # In this example grayscale_cam has only one image in the batch:                  \n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # Average all:\n",
    "    attention += grayscale_cam/len(val_dataloader)\n",
    "\n",
    "    image = denorm(img.cpu(), inputs.MEAN, inputs.STDV).cpu().numpy()\n",
    "    #visualize = show_cam_on_image(image, attention, use_rgb=True) \n",
    "    visualize = show_cam_on_image(image, grayscale_cam, use_rgb=True) #para visualizar uma imagem de cada vez\n",
    "    plt.imshow(visualize)\n",
    "    #break # para visualizar apenas 1 imagem de cada vez sem carregar o total de imagens do fold que você escolheu para visualizar (tem diferença)\n",
    "#para rodar o mapa de atenção médio tem que rodar essa celula sem o break acima, porque ele vai fazer uma varredura com todas imagen da validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a4286-0bee-4888-86ea-5cdbc364ebb1",
   "metadata": {},
   "source": [
    "## EigenCAM médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07001a70-6d65-4bce-9e53-b0f375d1a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = show_cam_on_image(image, attention, use_rgb=True)\n",
    "plt.imshow(visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a7920-1a97-4b10-a49b-d87e43cb4c5b",
   "metadata": {},
   "source": [
    "## Pontos máximos de atenção do EigenCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722291c0-3262-4131-93b9-d3ab456e284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pontos máximos de atenção sem uma imagem específica \n",
    "import numpy as np\n",
    "media = attention.squeeze().cpu().numpy()\n",
    "plt.imshow(media)\n",
    "np.where(media == media.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0193157-9340-42ac-9640-87102f1a4b35",
   "metadata": {},
   "source": [
    "## GradCAMPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c9c39-319d-4bba-9762-72ce38c47f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.features[-1]] ## this is the last layer for efficientnet b1 e b0\n",
    "cam = GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "attention = torch.zeros((224,224))  #(224,224) para a b0 e (240,240) para a b1\n",
    "model = model.cuda()\n",
    "for idx, (img, label) in tqdm(enumerate(val_dataloader)):\n",
    "    img, label = img.cuda(), label.cuda()\n",
    "    preds = model(img)\n",
    "    prediction = torch.argmax(preds).item()\n",
    "    ground_truth = label.item()\n",
    "    \n",
    "    grayscale_cam = cam(input_tensor=img, targets=None)\n",
    "\n",
    "     # In this example grayscale_cam has only one image in the batch:                  \n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # Average all:\n",
    "    attention += grayscale_cam/len(val_dataloader)\n",
    "\n",
    "    image = denorm(img.cpu(), inputs.MEAN, inputs.STDV).cpu().numpy()\n",
    "    #visualize = show_cam_on_image(image, attention, use_rgb=True) \n",
    "    visualize = show_cam_on_image(image, grayscale_cam, use_rgb=True) #para visualizar uma imagem de cada vez\n",
    "    plt.imshow(visualize)\n",
    "    #break # para visualizar apenas 1 imagem de cada vez sem carregar o total de imagens do fold que você escolheu para visualizar (tem diferença)\n",
    "#para rodar o mapa de atenção médio tem que rodar essa celula sem o break acima, porque ele vai fazer uma varredura com todas imagen da validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94957019-6a48-479c-92ee-7e1cbf5e9656",
   "metadata": {},
   "source": [
    "## GradCAMPlusPlus médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429671f0-2b63-465c-9829-f0d21d77598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = show_cam_on_image(image, attention, use_rgb=True)\n",
    "plt.imshow(visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91262579-33c3-4081-8062-4e81535483d0",
   "metadata": {},
   "source": [
    "## Pontos máximos de atenção do GradCAMPlusPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c7f9b-edb3-4e69-84f2-999fa0499da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pontos máximos de atenção sem uma imagem específica \n",
    "import numpy as np\n",
    "media = attention.squeeze().cpu().numpy()\n",
    "plt.imshow(media)\n",
    "np.where(media == media.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317b8ea-6733-4bfa-ab71-22ae51b033ed",
   "metadata": {},
   "source": [
    "## Aplicando o retângulo preto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f914b0c-bb35-44c8-9f50-5625b29d2981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_orig = denorm(img[0].cpu(), Inputs.MEAN, Inputs.STDV)\n",
    "plt.imshow(img_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce568fd5-a9cf-4e19-b369-39a63128bc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#img_orig[100:150, 62:162, :] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
