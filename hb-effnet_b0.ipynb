{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af75a1f7-445a-40aa-9f73-ddd8d58ebe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from configs import Inputs\n",
    "import utils.augmentations\n",
    "from utils.data import FullRadiographSexDataset\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601b2a6a-500f-492d-bef4-6813dadd4601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Batch size: 4\n",
      "Classes: 2\n",
      "Dir: /home/david/Documents/iVision/patch-1\n",
      "Epochs: 30\n",
      "Number of training examples: 120\n",
      "Number of validation examples: 40\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\" if torch.cuda.device_count() < 1 else \"cuda:0\")\n",
    "batch_size = 4\n",
    "CLASSES = 2\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 30\n",
    "N_TRAIN_EXAMPLES = batch_size * 30\n",
    "N_VALID_EXAMPLES = batch_size * 10\n",
    "gpus_per_trial = 1\n",
    "cpus_per_trial = 2\n",
    "\n",
    "print(f\"Device: {DEVICE}\\nBatch size: {batch_size}\\nClasses: {CLASSES}\\n\\\n",
    "Dir: {DIR}\\nEpochs: {EPOCHS}\\n\\\n",
    "Number of training examples: {N_TRAIN_EXAMPLES}\\n\\\n",
    "Number of validation examples: {N_VALID_EXAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2b8fd7-6b66-47c1-bea0-6a2ae746d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b0(weights=torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078667af-80b6-4af6-8917-37dc4a45bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "img_size = 224\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.Resize((img_size,img_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514228f8-0727-45d9-8301-9cb7059b7679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only horizontal flip augmentation.\n",
      "Using only horizontal flip augmentation.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from configs import Inputs\n",
    "from utils.augmentations import get_transforms\n",
    "from utils.data import FullRadiographSexDataset\n",
    "\n",
    "val_dataset = FullRadiographSexDataset(root_dir=Inputs.DATASET_DIR,\n",
    "                                       fold_nums=Inputs().val_folds,\n",
    "                                       transforms=get_transforms(Inputs(), subset=[\"val\"]))\n",
    "\n",
    "train_dataset = FullRadiographSexDataset(root_dir=Inputs.DATASET_DIR,\n",
    "                                         fold_nums=Inputs().train_folds,\n",
    "                                         transforms=get_transforms(Inputs(), subset=[\"train\"]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867c5217-9b91-4576-af60-47a03d8ace24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.choice([1e-2, 1e-3, 1e-4, 1e-5]),\n",
    "    \"optimizer_name\": tune.choice([\"Adam\", \"AdamW\", \"SGD\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68dae266-b74f-4880-a342-3e467a4bc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels):\n",
    "    # convert outputs to the predicted classes\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    # compare predictions to true label\n",
    "    total = len(labels)\n",
    "    true_positives = (pred & labels.data.view_as(pred)).sum().item()\n",
    "    true_negatives = ((1 - pred) & (1 - labels).data.view_as(pred)).sum().item()\n",
    "    false_positives = (pred & (1 - labels).data.view_as(pred)).sum().item()\n",
    "    false_negatives = ((1 - pred) & labels.data.view_as(pred)).sum().item()\n",
    "\n",
    "    return {\n",
    "        'tp': true_positives,\n",
    "        'tn': true_negatives,\n",
    "        'fp': false_positives,\n",
    "        'fn': false_negatives,\n",
    "        'total': total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f61568-b11e-4f0f-a43f-596db3c1be60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "\n",
    "    # Gerar o modelo\n",
    "    model = torchvision.models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Gerar optimizer\n",
    "    optimizer_name = config['optimizer_name']\n",
    "    lr = config[\"lr\"]\n",
    "    \n",
    "    print(\"opt_name:\", optimizer_name, \"\\nlr:\", lr)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, valid_loader = train_dataloader, val_dataloader\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss, total = 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for epoch in range(5):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # sum up the loss\n",
    "            running_loss += loss.item() * len(inputs)\n",
    "\n",
    "            metrics = compute_metrics(outputs, labels)\n",
    "            tp += metrics['tp']\n",
    "            tn += metrics['tn']\n",
    "            fp += metrics['fp']\n",
    "            fn += metrics['fn']\n",
    "            total += metrics['total']\n",
    "            \n",
    "        if total != 0:\n",
    "            accuracy = (tp + tn) / total\n",
    "        else:\n",
    "            accuracy = 0\n",
    "        \n",
    "        if (tp + fp) != 0:\n",
    "            precision = tp / (tp + fp)\n",
    "        else:\n",
    "            precision = 0\n",
    "        \n",
    "        if (tp+fn) != 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        else:\n",
    "            recall = 0\n",
    "        \n",
    "        if (2 * tp + fp + fn) != 0:\n",
    "            f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "        else:\n",
    "            f1 = 0\n",
    "\n",
    "        print(f'Train precision: {precision:.4f}')\n",
    "        print(f'Train recall: {recall:.4f}')\n",
    "        print(f'Train F1: {f1:.4f}')\n",
    "        print(f'Training loss: {running_loss / len(train_dataloader):.5f}')\n",
    "        print(f'Training accuracy: {100*accuracy:.2f} (%)')\n",
    "        \n",
    "        # Validation loss\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            running_loss, total = 0, 0\n",
    "            tp, tn, fp, fn = 0, 0, 0, 0 \n",
    "            \n",
    "            for i, data in enumerate(val_dataloader, 0):\n",
    "                model.eval()\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # sum up the loss\n",
    "                running_loss += loss.item() * len(inputs)\n",
    "\n",
    "                metrics_dict = compute_metrics(outputs, labels)\n",
    "                tp += metrics_dict['tp']\n",
    "                tn += metrics_dict['tn']\n",
    "                fp += metrics_dict['fp']\n",
    "                fn += metrics_dict['fn']\n",
    "                total += metrics_dict['total']\n",
    "\n",
    "            if total != 0:\n",
    "                accuracy = (tp + tn) / total\n",
    "            else:\n",
    "                accuracy = 0\n",
    "                \n",
    "            if (tp+fp) != 0:\n",
    "                precision = tp / (tp + fp)\n",
    "            else:\n",
    "                precision = 0\n",
    "                \n",
    "            if (tp+fn) != 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            else:\n",
    "                recall = 0\n",
    "                \n",
    "            if (2 * tp + fp + fn) != 0:\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "            else:\n",
    "                f1 = 0\n",
    "\n",
    "            val_loss=running_loss / len(val_dataloader)\n",
    "            print(f'Validation loss: {val_loss:.5f}')\n",
    "            print(f'Validation accuracy: {100*accuracy:.2f} (%)') \n",
    "            print(f'Validation precision: {precision:.4f}')\n",
    "            print(f'Validation recall: {recall:.4f}')\n",
    "            print(f'Validation F1: {f1:.4f}')\n",
    "            \n",
    "            tune.report(loss=val_loss, accuracy=accuracy)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "029019d3-2d8d-4f51-8464-0cf189fe94e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 13:00:18,014\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-22 14:53:46</td></tr>\n",
       "<tr><td>Running for: </td><td>1 days, 01:53:27.35</td></tr>\n",
       "<tr><td>Memory:      </td><td>5.9/11.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=120<br>Bracket: Iter 4.000: -2.1635178857896893 | Iter 2.000: -2.753374115626017 | Iter 1.000: -2.653029493159718<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.82 GiB heap, 0.0/2.41 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th>optimizer_name  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">         loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_b426b_00000</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1731.72 </td><td style=\"text-align: right;\">  2.74037    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00001</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1633.26 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00002</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.148</td><td style=\"text-align: right;\">  1.90116e+17</td><td style=\"text-align: right;\">  0.539388</td></tr>\n",
       "<tr><td>objective_b426b_00003</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1553.74 </td><td style=\"text-align: right;\">  2.38334    </td><td style=\"text-align: right;\">  0.734013</td></tr>\n",
       "<tr><td>objective_b426b_00004</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.898</td><td style=\"text-align: right;\"> 11.9024     </td><td style=\"text-align: right;\">  0.688601</td></tr>\n",
       "<tr><td>objective_b426b_00005</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.926</td><td style=\"text-align: right;\">  2.76916    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00006</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.698</td><td style=\"text-align: right;\"> 38.253      </td><td style=\"text-align: right;\">  0.796108</td></tr>\n",
       "<tr><td>objective_b426b_00007</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1605.47 </td><td style=\"text-align: right;\">  2.71086    </td><td style=\"text-align: right;\">  0.791474</td></tr>\n",
       "<tr><td>objective_b426b_00008</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         645.764</td><td style=\"text-align: right;\">  2.76802    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00009</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1272.42 </td><td style=\"text-align: right;\">  2.75434    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00010</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.625</td><td style=\"text-align: right;\">  8.58652    </td><td style=\"text-align: right;\">  0.57924 </td></tr>\n",
       "<tr><td>objective_b426b_00011</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.871</td><td style=\"text-align: right;\">  6.25698    </td><td style=\"text-align: right;\">  0.52456 </td></tr>\n",
       "<tr><td>objective_b426b_00012</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         320.009</td><td style=\"text-align: right;\"> 23.9792     </td><td style=\"text-align: right;\">  0.533828</td></tr>\n",
       "<tr><td>objective_b426b_00013</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         647.969</td><td style=\"text-align: right;\">  1.43489e+16</td><td style=\"text-align: right;\">  0.505097</td></tr>\n",
       "<tr><td>objective_b426b_00014</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         645.227</td><td style=\"text-align: right;\">  2.76957    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00015</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.751</td><td style=\"text-align: right;\">  2.76787    </td><td style=\"text-align: right;\">  0.518999</td></tr>\n",
       "<tr><td>objective_b426b_00016</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.926</td><td style=\"text-align: right;\">  6.09648    </td><td style=\"text-align: right;\">  0.479147</td></tr>\n",
       "<tr><td>objective_b426b_00017</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         645.866</td><td style=\"text-align: right;\">  6.73042    </td><td style=\"text-align: right;\">  0.428174</td></tr>\n",
       "<tr><td>objective_b426b_00018</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.295</td><td style=\"text-align: right;\">  3.28694    </td><td style=\"text-align: right;\">  0.459685</td></tr>\n",
       "<tr><td>objective_b426b_00019</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.757</td><td style=\"text-align: right;\"> 57.2494     </td><td style=\"text-align: right;\">  0.55051 </td></tr>\n",
       "<tr><td>objective_b426b_00020</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         329.874</td><td style=\"text-align: right;\">  8.30964    </td><td style=\"text-align: right;\">  0.712697</td></tr>\n",
       "<tr><td>objective_b426b_00021</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1282.11 </td><td style=\"text-align: right;\">  4.20327    </td><td style=\"text-align: right;\">  0.690454</td></tr>\n",
       "<tr><td>objective_b426b_00022</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.566</td><td style=\"text-align: right;\">  3.30741    </td><td style=\"text-align: right;\">  0.500463</td></tr>\n",
       "<tr><td>objective_b426b_00023</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.541</td><td style=\"text-align: right;\">  9.18313    </td><td style=\"text-align: right;\">  0.445783</td></tr>\n",
       "<tr><td>objective_b426b_00024</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1598.2  </td><td style=\"text-align: right;\">  2.19792    </td><td style=\"text-align: right;\">  0.765524</td></tr>\n",
       "<tr><td>objective_b426b_00025</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1644.81 </td><td style=\"text-align: right;\">  1.68863    </td><td style=\"text-align: right;\">  0.810936</td></tr>\n",
       "<tr><td>objective_b426b_00026</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1543.8  </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00027</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1282.96 </td><td style=\"text-align: right;\">  2.7536     </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00028</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1600.26 </td><td style=\"text-align: right;\">  3.71645    </td><td style=\"text-align: right;\">  0.736793</td></tr>\n",
       "<tr><td>objective_b426b_00029</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1606.18 </td><td style=\"text-align: right;\">  2.88262    </td><td style=\"text-align: right;\">  0.721965</td></tr>\n",
       "<tr><td>objective_b426b_00030</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.672</td><td style=\"text-align: right;\"> 12.0074     </td><td style=\"text-align: right;\">  0.442076</td></tr>\n",
       "<tr><td>objective_b426b_00031</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1542.59 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00032</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.052</td><td style=\"text-align: right;\">  5.00105    </td><td style=\"text-align: right;\">  0.493976</td></tr>\n",
       "<tr><td>objective_b426b_00033</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         319.111</td><td style=\"text-align: right;\"> 32.4152     </td><td style=\"text-align: right;\">  0.621872</td></tr>\n",
       "<tr><td>objective_b426b_00034</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.209</td><td style=\"text-align: right;\">  7.68098    </td><td style=\"text-align: right;\">  0.537535</td></tr>\n",
       "<tr><td>objective_b426b_00035</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1282.25 </td><td style=\"text-align: right;\">  2.73863    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00036</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1542.72 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00037</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         331.361</td><td style=\"text-align: right;\">  3.38524    </td><td style=\"text-align: right;\">  0.554217</td></tr>\n",
       "<tr><td>objective_b426b_00038</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1556.53 </td><td style=\"text-align: right;\">  2.06934    </td><td style=\"text-align: right;\">  0.788693</td></tr>\n",
       "<tr><td>objective_b426b_00039</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         348.604</td><td style=\"text-align: right;\">  2.89869    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00040</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         350.951</td><td style=\"text-align: right;\">  3.05635    </td><td style=\"text-align: right;\">  0.60241 </td></tr>\n",
       "<tr><td>objective_b426b_00041</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         321.015</td><td style=\"text-align: right;\">  2.7707     </td><td style=\"text-align: right;\">  0.520853</td></tr>\n",
       "<tr><td>objective_b426b_00042</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         329.727</td><td style=\"text-align: right;\">  2.89042    </td><td style=\"text-align: right;\">  0.539388</td></tr>\n",
       "<tr><td>objective_b426b_00043</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1556.07 </td><td style=\"text-align: right;\">  1.87648    </td><td style=\"text-align: right;\">  0.794254</td></tr>\n",
       "<tr><td>objective_b426b_00044</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1605.42 </td><td style=\"text-align: right;\">  1.49808    </td><td style=\"text-align: right;\">  0.851715</td></tr>\n",
       "<tr><td>objective_b426b_00045</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1542.14 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00046</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         651.55 </td><td style=\"text-align: right;\">  2.75521    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00047</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.865</td><td style=\"text-align: right;\">  3.77224    </td><td style=\"text-align: right;\">  0.599629</td></tr>\n",
       "<tr><td>objective_b426b_00048</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1546.34 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00049</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.628</td><td style=\"text-align: right;\">  4.4962     </td><td style=\"text-align: right;\">  0.629286</td></tr>\n",
       "<tr><td>objective_b426b_00050</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1533.43 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00051</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         325.944</td><td style=\"text-align: right;\">  8.97619    </td><td style=\"text-align: right;\">  0.771084</td></tr>\n",
       "<tr><td>objective_b426b_00052</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.424</td><td style=\"text-align: right;\">  2.99848    </td><td style=\"text-align: right;\">  0.60241 </td></tr>\n",
       "<tr><td>objective_b426b_00053</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         641.183</td><td style=\"text-align: right;\">  2.75432    </td><td style=\"text-align: right;\">  0.566265</td></tr>\n",
       "<tr><td>objective_b426b_00054</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         316.989</td><td style=\"text-align: right;\"> 21.3383     </td><td style=\"text-align: right;\">  0.600556</td></tr>\n",
       "<tr><td>objective_b426b_00055</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.1  </td><td style=\"text-align: right;\">  3.35923    </td><td style=\"text-align: right;\">  0.682113</td></tr>\n",
       "<tr><td>objective_b426b_00056</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         642.111</td><td style=\"text-align: right;\">  2.74651    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00057</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         640.382</td><td style=\"text-align: right;\">  2.89074    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00058</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1597.84 </td><td style=\"text-align: right;\">  3.54394    </td><td style=\"text-align: right;\">  0.728452</td></tr>\n",
       "<tr><td>objective_b426b_00059</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1279.57 </td><td style=\"text-align: right;\">  3.52361    </td><td style=\"text-align: right;\">  0.725672</td></tr>\n",
       "<tr><td>objective_b426b_00060</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1277.89 </td><td style=\"text-align: right;\">  2.2912     </td><td style=\"text-align: right;\">  0.729379</td></tr>\n",
       "<tr><td>objective_b426b_00061</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         317.182</td><td style=\"text-align: right;\"> 19.9367     </td><td style=\"text-align: right;\">  0.544949</td></tr>\n",
       "<tr><td>objective_b426b_00062</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1604.37 </td><td style=\"text-align: right;\">  2.17366    </td><td style=\"text-align: right;\">  0.808156</td></tr>\n",
       "<tr><td>objective_b426b_00063</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.875</td><td style=\"text-align: right;\">  3.16015    </td><td style=\"text-align: right;\">  0.563485</td></tr>\n",
       "<tr><td>objective_b426b_00064</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.911</td><td style=\"text-align: right;\">  4.01249    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00065</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         317.808</td><td style=\"text-align: right;\">  2.80515    </td><td style=\"text-align: right;\">  0.460612</td></tr>\n",
       "<tr><td>objective_b426b_00066</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         329.372</td><td style=\"text-align: right;\">  3.96804    </td><td style=\"text-align: right;\">  0.732159</td></tr>\n",
       "<tr><td>objective_b426b_00067</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.842</td><td style=\"text-align: right;\"> 27.2605     </td><td style=\"text-align: right;\">  0.565338</td></tr>\n",
       "<tr><td>objective_b426b_00068</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1540.32 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00069</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.936</td><td style=\"text-align: right;\"> 12.276      </td><td style=\"text-align: right;\">  0.748842</td></tr>\n",
       "<tr><td>objective_b426b_00070</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1279.19 </td><td style=\"text-align: right;\">  2.44306    </td><td style=\"text-align: right;\">  0.658943</td></tr>\n",
       "<tr><td>objective_b426b_00071</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         642.713</td><td style=\"text-align: right;\">  2.77055    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00072</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         335.521</td><td style=\"text-align: right;\">  3.40449    </td><td style=\"text-align: right;\">  0.577386</td></tr>\n",
       "<tr><td>objective_b426b_00073</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.719</td><td style=\"text-align: right;\">  3.99003    </td><td style=\"text-align: right;\">  0.701576</td></tr>\n",
       "<tr><td>objective_b426b_00074</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1533.59 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00075</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1591.31 </td><td style=\"text-align: right;\">  3.2462     </td><td style=\"text-align: right;\">  0.705283</td></tr>\n",
       "<tr><td>objective_b426b_00076</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         640.757</td><td style=\"text-align: right;\">  2.74014    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00077</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1599.41 </td><td style=\"text-align: right;\">  2.32252    </td><td style=\"text-align: right;\">  0.784986</td></tr>\n",
       "<tr><td>objective_b426b_00078</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.596</td><td style=\"text-align: right;\">  2.77151    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00079</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1528.24 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00080</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         324.854</td><td style=\"text-align: right;\">  2.91728    </td><td style=\"text-align: right;\">  0.617238</td></tr>\n",
       "<tr><td>objective_b426b_00081</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         640.793</td><td style=\"text-align: right;\">  2.74612    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00082</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.346</td><td style=\"text-align: right;\">  2.78829    </td><td style=\"text-align: right;\">  0.557924</td></tr>\n",
       "<tr><td>objective_b426b_00083</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         325.997</td><td style=\"text-align: right;\">  3.89144    </td><td style=\"text-align: right;\">  0.52456 </td></tr>\n",
       "<tr><td>objective_b426b_00084</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         646.124</td><td style=\"text-align: right;\">  2.76296    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00085</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         644.743</td><td style=\"text-align: right;\">  2.76043    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00086</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.751</td><td style=\"text-align: right;\">  5.69013    </td><td style=\"text-align: right;\">  0.442076</td></tr>\n",
       "<tr><td>objective_b426b_00087</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         647.157</td><td style=\"text-align: right;\">  2.77181    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00088</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         641.205</td><td style=\"text-align: right;\">  2.76483    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00089</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         639.944</td><td style=\"text-align: right;\">  2.75931    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00090</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1533.85 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00091</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1596.86 </td><td style=\"text-align: right;\">  3.57014    </td><td style=\"text-align: right;\">  0.720111</td></tr>\n",
       "<tr><td>objective_b426b_00092</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         317.496</td><td style=\"text-align: right;\">  2.74466    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00093</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         319.022</td><td style=\"text-align: right;\">  2.77653    </td><td style=\"text-align: right;\">  0.520853</td></tr>\n",
       "<tr><td>objective_b426b_00094</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         641.186</td><td style=\"text-align: right;\">  2.76715    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00095</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.379</td><td style=\"text-align: right;\">  9.06037    </td><td style=\"text-align: right;\">  0.435589</td></tr>\n",
       "<tr><td>objective_b426b_00096</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         640.527</td><td style=\"text-align: right;\">  2.76608    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00097</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         646.358</td><td style=\"text-align: right;\">  2.76436    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00098</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1270.96 </td><td style=\"text-align: right;\">  2.57574    </td><td style=\"text-align: right;\">  0.777572</td></tr>\n",
       "<tr><td>objective_b426b_00099</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.105</td><td style=\"text-align: right;\">  2.91287    </td><td style=\"text-align: right;\">  0.538462</td></tr>\n",
       "<tr><td>objective_b426b_00100</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         318.226</td><td style=\"text-align: right;\">  2.73321    </td><td style=\"text-align: right;\">  0.559778</td></tr>\n",
       "<tr><td>objective_b426b_00101</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1539.18 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00102</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         326.265</td><td style=\"text-align: right;\"> 19.1544     </td><td style=\"text-align: right;\">  0.565338</td></tr>\n",
       "<tr><td>objective_b426b_00103</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         650.796</td><td style=\"text-align: right;\">  2.76561    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00104</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1546.99 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00105</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>SGD             </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1546.64 </td><td style=\"text-align: right;\">nan          </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00106</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         328.943</td><td style=\"text-align: right;\"> 25.1868     </td><td style=\"text-align: right;\">  0.756256</td></tr>\n",
       "<tr><td>objective_b426b_00107</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         651.233</td><td style=\"text-align: right;\">  2.76869    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00108</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         330.162</td><td style=\"text-align: right;\">  4.60248    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00109</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.001 </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         327.723</td><td style=\"text-align: right;\">  2.78147    </td><td style=\"text-align: right;\">  0.614458</td></tr>\n",
       "<tr><td>objective_b426b_00110</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         332.952</td><td style=\"text-align: right;\">  4.12129    </td><td style=\"text-align: right;\">  0.447637</td></tr>\n",
       "<tr><td>objective_b426b_00111</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         329.982</td><td style=\"text-align: right;\"> 10.7771     </td><td style=\"text-align: right;\">  0.706209</td></tr>\n",
       "<tr><td>objective_b426b_00112</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>AdamW           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         330.172</td><td style=\"text-align: right;\">  3.40758    </td><td style=\"text-align: right;\">  0.531974</td></tr>\n",
       "<tr><td>objective_b426b_00113</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.01  </td><td>Adam            </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         331.27 </td><td style=\"text-align: right;\">  3.63801    </td><td style=\"text-align: right;\">  0.62836 </td></tr>\n",
       "<tr><td>objective_b426b_00114</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         320.83 </td><td style=\"text-align: right;\">  2.74768    </td><td style=\"text-align: right;\">  0.542169</td></tr>\n",
       "<tr><td>objective_b426b_00115</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         322.325</td><td style=\"text-align: right;\">  2.7628     </td><td style=\"text-align: right;\">  0.511585</td></tr>\n",
       "<tr><td>objective_b426b_00116</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>AdamW           </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         653.732</td><td style=\"text-align: right;\">  2.75337    </td><td style=\"text-align: right;\">  0.564411</td></tr>\n",
       "<tr><td>objective_b426b_00117</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>Adam            </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         652.407</td><td style=\"text-align: right;\"> 50.0007     </td><td style=\"text-align: right;\">  0.434662</td></tr>\n",
       "<tr><td>objective_b426b_00118</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">0.0001</td><td>SGD             </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         317.591</td><td style=\"text-align: right;\"> 13.0908     </td><td style=\"text-align: right;\">  0.555144</td></tr>\n",
       "<tr><td>objective_b426b_00119</td><td>TERMINATED</td><td>192.168.1.20:86030</td><td style=\"text-align: right;\">1e-05 </td><td>Adam            </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        1599.29 </td><td style=\"text-align: right;\">  1.93371    </td><td style=\"text-align: right;\">  0.783133</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6202\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4856\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5447\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.54237\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.89 (%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">         loss</th><th>node_ip     </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_b426b_00000</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_13-29-14</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.74037    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1731.72 </td><td style=\"text-align: right;\">           344.134</td><td style=\"text-align: right;\">      1731.72 </td><td style=\"text-align: right;\"> 1674318554</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00000</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00001</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_13-56-28</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1633.26 </td><td style=\"text-align: right;\">           306.827</td><td style=\"text-align: right;\">      1633.26 </td><td style=\"text-align: right;\"> 1674320188</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00001</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00002</td><td style=\"text-align: right;\">  0.539388</td><td>2023-01-21_14-07-12</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  1.90116e+17</td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.148</td><td style=\"text-align: right;\">           316.762</td><td style=\"text-align: right;\">       644.148</td><td style=\"text-align: right;\"> 1674320832</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00002</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00003</td><td style=\"text-align: right;\">  0.734013</td><td>2023-01-21_14-33-05</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.38334    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1553.74 </td><td style=\"text-align: right;\">           308.731</td><td style=\"text-align: right;\">      1553.74 </td><td style=\"text-align: right;\"> 1674322385</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00003</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00004</td><td style=\"text-align: right;\">  0.688601</td><td>2023-01-21_14-38-33</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 11.9024     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.898</td><td style=\"text-align: right;\">           327.898</td><td style=\"text-align: right;\">       327.898</td><td style=\"text-align: right;\"> 1674322713</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00004</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00005</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_14-49-18</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76916    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.926</td><td style=\"text-align: right;\">           316.899</td><td style=\"text-align: right;\">       644.926</td><td style=\"text-align: right;\"> 1674323358</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00005</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00006</td><td style=\"text-align: right;\">  0.796108</td><td>2023-01-21_15-00-03</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 38.253      </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.698</td><td style=\"text-align: right;\">           316.86 </td><td style=\"text-align: right;\">       644.698</td><td style=\"text-align: right;\"> 1674324003</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00006</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00007</td><td style=\"text-align: right;\">  0.791474</td><td>2023-01-21_15-26-49</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.71086    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1605.47 </td><td style=\"text-align: right;\">           318.918</td><td style=\"text-align: right;\">      1605.47 </td><td style=\"text-align: right;\"> 1674325609</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00007</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00008</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_15-37-34</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76802    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             645.764</td><td style=\"text-align: right;\">           317.51 </td><td style=\"text-align: right;\">       645.764</td><td style=\"text-align: right;\"> 1674326254</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00008</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00009</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_15-58-47</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.75434    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1272.42 </td><td style=\"text-align: right;\">           314.947</td><td style=\"text-align: right;\">      1272.42 </td><td style=\"text-align: right;\"> 1674327527</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00009</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00010</td><td style=\"text-align: right;\">  0.57924 </td><td>2023-01-21_16-04-05</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  8.58652    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.625</td><td style=\"text-align: right;\">           318.625</td><td style=\"text-align: right;\">       318.625</td><td style=\"text-align: right;\"> 1674327845</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00010</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00011</td><td style=\"text-align: right;\">  0.52456 </td><td>2023-01-21_16-09-24</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  6.25698    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.871</td><td style=\"text-align: right;\">           318.871</td><td style=\"text-align: right;\">       318.871</td><td style=\"text-align: right;\"> 1674328164</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00011</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00012</td><td style=\"text-align: right;\">  0.533828</td><td>2023-01-21_16-14-44</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 23.9792     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             320.009</td><td style=\"text-align: right;\">           320.009</td><td style=\"text-align: right;\">       320.009</td><td style=\"text-align: right;\"> 1674328484</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00012</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00013</td><td style=\"text-align: right;\">  0.505097</td><td>2023-01-21_16-25-32</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  1.43489e+16</td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             647.969</td><td style=\"text-align: right;\">           318.082</td><td style=\"text-align: right;\">       647.969</td><td style=\"text-align: right;\"> 1674329132</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00013</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00014</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_16-36-18</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76957    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             645.227</td><td style=\"text-align: right;\">           317.305</td><td style=\"text-align: right;\">       645.227</td><td style=\"text-align: right;\"> 1674329778</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00014</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00015</td><td style=\"text-align: right;\">  0.518999</td><td>2023-01-21_16-41-36</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.76787    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.751</td><td style=\"text-align: right;\">           318.751</td><td style=\"text-align: right;\">       318.751</td><td style=\"text-align: right;\"> 1674330096</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00015</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00016</td><td style=\"text-align: right;\">  0.479147</td><td>2023-01-21_16-46-55</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  6.09648    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.926</td><td style=\"text-align: right;\">           318.926</td><td style=\"text-align: right;\">       318.926</td><td style=\"text-align: right;\"> 1674330415</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00016</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00017</td><td style=\"text-align: right;\">  0.428174</td><td>2023-01-21_16-57-41</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  6.73042    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             645.866</td><td style=\"text-align: right;\">           317.594</td><td style=\"text-align: right;\">       645.866</td><td style=\"text-align: right;\"> 1674331061</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00017</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00018</td><td style=\"text-align: right;\">  0.459685</td><td>2023-01-21_17-03-10</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.28694    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.295</td><td style=\"text-align: right;\">           328.295</td><td style=\"text-align: right;\">       328.295</td><td style=\"text-align: right;\"> 1674331390</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00018</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00019</td><td style=\"text-align: right;\">  0.55051 </td><td>2023-01-21_17-08-28</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 57.2494     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.757</td><td style=\"text-align: right;\">           318.757</td><td style=\"text-align: right;\">       318.757</td><td style=\"text-align: right;\"> 1674331708</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00019</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00020</td><td style=\"text-align: right;\">  0.712697</td><td>2023-01-21_17-13-58</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  8.30964    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             329.874</td><td style=\"text-align: right;\">           329.874</td><td style=\"text-align: right;\">       329.874</td><td style=\"text-align: right;\"> 1674332038</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00020</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00021</td><td style=\"text-align: right;\">  0.690454</td><td>2023-01-21_17-35-20</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  4.20327    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1282.11 </td><td style=\"text-align: right;\">           318.151</td><td style=\"text-align: right;\">      1282.11 </td><td style=\"text-align: right;\"> 1674333320</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00021</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00022</td><td style=\"text-align: right;\">  0.500463</td><td>2023-01-21_17-40-48</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.30741    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.566</td><td style=\"text-align: right;\">           327.566</td><td style=\"text-align: right;\">       327.566</td><td style=\"text-align: right;\"> 1674333648</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00022</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00023</td><td style=\"text-align: right;\">  0.445783</td><td>2023-01-21_17-46-17</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  9.18313    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.541</td><td style=\"text-align: right;\">           328.541</td><td style=\"text-align: right;\">       328.541</td><td style=\"text-align: right;\"> 1674333977</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00023</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00024</td><td style=\"text-align: right;\">  0.765524</td><td>2023-01-21_18-12-55</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.19792    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1598.2  </td><td style=\"text-align: right;\">           317.694</td><td style=\"text-align: right;\">      1598.2  </td><td style=\"text-align: right;\"> 1674335575</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00024</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00025</td><td style=\"text-align: right;\">  0.810936</td><td>2023-01-21_18-40-20</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  1.68863    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1644.81 </td><td style=\"text-align: right;\">           317.699</td><td style=\"text-align: right;\">      1644.81 </td><td style=\"text-align: right;\"> 1674337220</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00025</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00026</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_19-06-03</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1543.8  </td><td style=\"text-align: right;\">           305.998</td><td style=\"text-align: right;\">      1543.8  </td><td style=\"text-align: right;\"> 1674338763</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00026</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00027</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_19-27-26</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.7536     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1282.96 </td><td style=\"text-align: right;\">           319.026</td><td style=\"text-align: right;\">      1282.96 </td><td style=\"text-align: right;\"> 1674340046</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00027</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00028</td><td style=\"text-align: right;\">  0.736793</td><td>2023-01-21_19-54-07</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  3.71645    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1600.26 </td><td style=\"text-align: right;\">           316.186</td><td style=\"text-align: right;\">      1600.26 </td><td style=\"text-align: right;\"> 1674341647</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00028</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00029</td><td style=\"text-align: right;\">  0.721965</td><td>2023-01-21_20-20-53</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.88262    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1606.18 </td><td style=\"text-align: right;\">           317.953</td><td style=\"text-align: right;\">      1606.18 </td><td style=\"text-align: right;\"> 1674343253</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00029</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00030</td><td style=\"text-align: right;\">  0.442076</td><td>2023-01-21_20-26-22</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 12.0074     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.672</td><td style=\"text-align: right;\">           328.672</td><td style=\"text-align: right;\">       328.672</td><td style=\"text-align: right;\"> 1674343582</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00030</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00031</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_20-52-04</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1542.59 </td><td style=\"text-align: right;\">           305.391</td><td style=\"text-align: right;\">      1542.59 </td><td style=\"text-align: right;\"> 1674345124</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00031</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00032</td><td style=\"text-align: right;\">  0.493976</td><td>2023-01-21_20-57-31</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  5.00105    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.052</td><td style=\"text-align: right;\">           327.052</td><td style=\"text-align: right;\">       327.052</td><td style=\"text-align: right;\"> 1674345451</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00032</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00033</td><td style=\"text-align: right;\">  0.621872</td><td>2023-01-21_21-02-51</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 32.4152     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             319.111</td><td style=\"text-align: right;\">           319.111</td><td style=\"text-align: right;\">       319.111</td><td style=\"text-align: right;\"> 1674345771</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00033</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00034</td><td style=\"text-align: right;\">  0.537535</td><td>2023-01-21_21-08-09</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  7.68098    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.209</td><td style=\"text-align: right;\">           318.209</td><td style=\"text-align: right;\">       318.209</td><td style=\"text-align: right;\"> 1674346089</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00034</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00035</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_21-29-31</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.73863    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1282.25 </td><td style=\"text-align: right;\">           317.13 </td><td style=\"text-align: right;\">      1282.25 </td><td style=\"text-align: right;\"> 1674347371</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00035</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00036</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_21-55-14</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1542.72 </td><td style=\"text-align: right;\">           305.8  </td><td style=\"text-align: right;\">      1542.72 </td><td style=\"text-align: right;\"> 1674348914</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00036</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00037</td><td style=\"text-align: right;\">  0.554217</td><td>2023-01-21_22-00-45</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.38524    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             331.361</td><td style=\"text-align: right;\">           331.361</td><td style=\"text-align: right;\">       331.361</td><td style=\"text-align: right;\"> 1674349245</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00037</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00038</td><td style=\"text-align: right;\">  0.788693</td><td>2023-01-21_22-26-42</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.06934    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1556.53 </td><td style=\"text-align: right;\">           308.818</td><td style=\"text-align: right;\">      1556.53 </td><td style=\"text-align: right;\"> 1674350802</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00038</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00039</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-21_22-32-30</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.89869    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             348.604</td><td style=\"text-align: right;\">           348.604</td><td style=\"text-align: right;\">       348.604</td><td style=\"text-align: right;\"> 1674351150</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00039</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00040</td><td style=\"text-align: right;\">  0.60241 </td><td>2023-01-21_22-38-21</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.05635    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             350.951</td><td style=\"text-align: right;\">           350.951</td><td style=\"text-align: right;\">       350.951</td><td style=\"text-align: right;\"> 1674351501</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00040</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00041</td><td style=\"text-align: right;\">  0.520853</td><td>2023-01-21_22-43-42</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.7707     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             321.015</td><td style=\"text-align: right;\">           321.015</td><td style=\"text-align: right;\">       321.015</td><td style=\"text-align: right;\"> 1674351822</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00041</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00042</td><td style=\"text-align: right;\">  0.539388</td><td>2023-01-21_22-49-12</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.89042    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             329.727</td><td style=\"text-align: right;\">           329.727</td><td style=\"text-align: right;\">       329.727</td><td style=\"text-align: right;\"> 1674352152</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00042</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00043</td><td style=\"text-align: right;\">  0.794254</td><td>2023-01-21_23-15-08</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  1.87648    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1556.07 </td><td style=\"text-align: right;\">           311.276</td><td style=\"text-align: right;\">      1556.07 </td><td style=\"text-align: right;\"> 1674353708</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00043</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00044</td><td style=\"text-align: right;\">  0.851715</td><td>2023-01-21_23-41-54</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  1.49808    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1605.42 </td><td style=\"text-align: right;\">           317.781</td><td style=\"text-align: right;\">      1605.42 </td><td style=\"text-align: right;\"> 1674355314</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00044</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00045</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_00-07-36</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1542.14 </td><td style=\"text-align: right;\">           305.651</td><td style=\"text-align: right;\">      1542.14 </td><td style=\"text-align: right;\"> 1674356856</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00045</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00046</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_00-18-28</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.75521    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             651.55 </td><td style=\"text-align: right;\">           322.927</td><td style=\"text-align: right;\">       651.55 </td><td style=\"text-align: right;\"> 1674357508</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00046</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00047</td><td style=\"text-align: right;\">  0.599629</td><td>2023-01-22_00-23-56</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.77224    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.865</td><td style=\"text-align: right;\">           328.865</td><td style=\"text-align: right;\">       328.865</td><td style=\"text-align: right;\"> 1674357836</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00047</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00048</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_00-49-43</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1546.34 </td><td style=\"text-align: right;\">           305.889</td><td style=\"text-align: right;\">      1546.34 </td><td style=\"text-align: right;\"> 1674359383</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00048</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00049</td><td style=\"text-align: right;\">  0.629286</td><td>2023-01-22_00-55-10</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  4.4962     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.628</td><td style=\"text-align: right;\">           326.628</td><td style=\"text-align: right;\">       326.628</td><td style=\"text-align: right;\"> 1674359710</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00049</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00050</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_01-20-43</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1533.43 </td><td style=\"text-align: right;\">           304.034</td><td style=\"text-align: right;\">      1533.43 </td><td style=\"text-align: right;\"> 1674361243</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00050</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00051</td><td style=\"text-align: right;\">  0.771084</td><td>2023-01-22_01-26-09</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  8.97619    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             325.944</td><td style=\"text-align: right;\">           325.944</td><td style=\"text-align: right;\">       325.944</td><td style=\"text-align: right;\"> 1674361569</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00051</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00052</td><td style=\"text-align: right;\">  0.60241 </td><td>2023-01-22_01-31-35</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.99848    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.424</td><td style=\"text-align: right;\">           326.424</td><td style=\"text-align: right;\">       326.424</td><td style=\"text-align: right;\"> 1674361895</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00052</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00053</td><td style=\"text-align: right;\">  0.566265</td><td>2023-01-22_01-42-17</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.75432    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             641.183</td><td style=\"text-align: right;\">           315.216</td><td style=\"text-align: right;\">       641.183</td><td style=\"text-align: right;\"> 1674362537</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00053</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00054</td><td style=\"text-align: right;\">  0.600556</td><td>2023-01-22_01-47-34</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 21.3383     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             316.989</td><td style=\"text-align: right;\">           316.989</td><td style=\"text-align: right;\">       316.989</td><td style=\"text-align: right;\"> 1674362854</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00054</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00055</td><td style=\"text-align: right;\">  0.682113</td><td>2023-01-22_01-53-00</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.35923    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.1  </td><td style=\"text-align: right;\">           326.1  </td><td style=\"text-align: right;\">       326.1  </td><td style=\"text-align: right;\"> 1674363180</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00055</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00056</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_02-03-42</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.74651    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             642.111</td><td style=\"text-align: right;\">           316.16 </td><td style=\"text-align: right;\">       642.111</td><td style=\"text-align: right;\"> 1674363822</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00056</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00057</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_02-14-22</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.89074    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             640.382</td><td style=\"text-align: right;\">           315.083</td><td style=\"text-align: right;\">       640.382</td><td style=\"text-align: right;\"> 1674364462</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00057</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00058</td><td style=\"text-align: right;\">  0.728452</td><td>2023-01-22_02-41-00</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  3.54394    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1597.84 </td><td style=\"text-align: right;\">           317.814</td><td style=\"text-align: right;\">      1597.84 </td><td style=\"text-align: right;\"> 1674366060</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00058</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00059</td><td style=\"text-align: right;\">  0.725672</td><td>2023-01-22_03-02-20</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  3.52361    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1279.57 </td><td style=\"text-align: right;\">           317.282</td><td style=\"text-align: right;\">      1279.57 </td><td style=\"text-align: right;\"> 1674367340</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00059</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00060</td><td style=\"text-align: right;\">  0.729379</td><td>2023-01-22_03-23-38</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.2912     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1277.89 </td><td style=\"text-align: right;\">           316.636</td><td style=\"text-align: right;\">      1277.89 </td><td style=\"text-align: right;\"> 1674368618</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00060</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00061</td><td style=\"text-align: right;\">  0.544949</td><td>2023-01-22_03-28-55</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 19.9367     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             317.182</td><td style=\"text-align: right;\">           317.182</td><td style=\"text-align: right;\">       317.182</td><td style=\"text-align: right;\"> 1674368935</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00061</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00062</td><td style=\"text-align: right;\">  0.808156</td><td>2023-01-22_03-55-40</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.17366    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1604.37 </td><td style=\"text-align: right;\">           318.937</td><td style=\"text-align: right;\">      1604.37 </td><td style=\"text-align: right;\"> 1674370540</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00062</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00063</td><td style=\"text-align: right;\">  0.563485</td><td>2023-01-22_04-06-24</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  3.16015    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.875</td><td style=\"text-align: right;\">           317.149</td><td style=\"text-align: right;\">       644.875</td><td style=\"text-align: right;\"> 1674371184</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00063</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00064</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_04-11-52</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  4.01249    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.911</td><td style=\"text-align: right;\">           327.911</td><td style=\"text-align: right;\">       327.911</td><td style=\"text-align: right;\"> 1674371512</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00064</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00065</td><td style=\"text-align: right;\">  0.460612</td><td>2023-01-22_04-17-10</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.80515    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             317.808</td><td style=\"text-align: right;\">           317.808</td><td style=\"text-align: right;\">       317.808</td><td style=\"text-align: right;\"> 1674371830</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00065</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00066</td><td style=\"text-align: right;\">  0.732159</td><td>2023-01-22_04-22-40</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.96804    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             329.372</td><td style=\"text-align: right;\">           329.372</td><td style=\"text-align: right;\">       329.372</td><td style=\"text-align: right;\"> 1674372160</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00066</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00067</td><td style=\"text-align: right;\">  0.565338</td><td>2023-01-22_04-28-09</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 27.2605     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.842</td><td style=\"text-align: right;\">           328.842</td><td style=\"text-align: right;\">       328.842</td><td style=\"text-align: right;\"> 1674372489</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00067</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00068</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_04-53-49</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1540.32 </td><td style=\"text-align: right;\">           305.343</td><td style=\"text-align: right;\">      1540.32 </td><td style=\"text-align: right;\"> 1674374029</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00068</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00069</td><td style=\"text-align: right;\">  0.748842</td><td>2023-01-22_04-59-18</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 12.276      </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.936</td><td style=\"text-align: right;\">           328.936</td><td style=\"text-align: right;\">       328.936</td><td style=\"text-align: right;\"> 1674374358</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00069</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00070</td><td style=\"text-align: right;\">  0.658943</td><td>2023-01-22_05-20-37</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.44306    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1279.19 </td><td style=\"text-align: right;\">           315.971</td><td style=\"text-align: right;\">      1279.19 </td><td style=\"text-align: right;\"> 1674375637</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00070</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00071</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_05-31-20</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.77055    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             642.713</td><td style=\"text-align: right;\">           315.931</td><td style=\"text-align: right;\">       642.713</td><td style=\"text-align: right;\"> 1674376280</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00071</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00072</td><td style=\"text-align: right;\">  0.577386</td><td>2023-01-22_05-36-56</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.40449    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             335.521</td><td style=\"text-align: right;\">           335.521</td><td style=\"text-align: right;\">       335.521</td><td style=\"text-align: right;\"> 1674376616</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00072</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00073</td><td style=\"text-align: right;\">  0.701576</td><td>2023-01-22_05-42-23</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.99003    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.719</td><td style=\"text-align: right;\">           327.719</td><td style=\"text-align: right;\">       327.719</td><td style=\"text-align: right;\"> 1674376943</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00073</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00074</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_06-07-57</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1533.59 </td><td style=\"text-align: right;\">           303.81 </td><td style=\"text-align: right;\">      1533.59 </td><td style=\"text-align: right;\"> 1674378477</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00074</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00075</td><td style=\"text-align: right;\">  0.705283</td><td>2023-01-22_06-34-28</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  3.2462     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1591.31 </td><td style=\"text-align: right;\">           315.82 </td><td style=\"text-align: right;\">      1591.31 </td><td style=\"text-align: right;\"> 1674380068</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00075</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00076</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_06-45-09</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.74014    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             640.757</td><td style=\"text-align: right;\">           315.205</td><td style=\"text-align: right;\">       640.757</td><td style=\"text-align: right;\"> 1674380709</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00076</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00077</td><td style=\"text-align: right;\">  0.784986</td><td>2023-01-22_07-11-49</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  2.32252    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1599.41 </td><td style=\"text-align: right;\">           317.758</td><td style=\"text-align: right;\">      1599.41 </td><td style=\"text-align: right;\"> 1674382309</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00077</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00078</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_07-22-33</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.77151    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.596</td><td style=\"text-align: right;\">           316.799</td><td style=\"text-align: right;\">       644.596</td><td style=\"text-align: right;\"> 1674382953</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00078</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00079</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_07-48-02</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1528.24 </td><td style=\"text-align: right;\">           303.872</td><td style=\"text-align: right;\">      1528.24 </td><td style=\"text-align: right;\"> 1674384482</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00079</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00080</td><td style=\"text-align: right;\">  0.617238</td><td>2023-01-22_07-53-26</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.91728    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             324.854</td><td style=\"text-align: right;\">           324.854</td><td style=\"text-align: right;\">       324.854</td><td style=\"text-align: right;\"> 1674384806</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00080</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00081</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_08-04-07</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.74612    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             640.793</td><td style=\"text-align: right;\">           314.739</td><td style=\"text-align: right;\">       640.793</td><td style=\"text-align: right;\"> 1674385447</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00081</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00082</td><td style=\"text-align: right;\">  0.557924</td><td>2023-01-22_08-09-34</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.78829    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.346</td><td style=\"text-align: right;\">           326.346</td><td style=\"text-align: right;\">       326.346</td><td style=\"text-align: right;\"> 1674385774</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00082</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00083</td><td style=\"text-align: right;\">  0.52456 </td><td>2023-01-22_08-15-00</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.89144    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             325.997</td><td style=\"text-align: right;\">           325.997</td><td style=\"text-align: right;\">       325.997</td><td style=\"text-align: right;\"> 1674386100</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00083</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00084</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_08-25-46</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76296    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             646.124</td><td style=\"text-align: right;\">           316.623</td><td style=\"text-align: right;\">       646.124</td><td style=\"text-align: right;\"> 1674386746</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00084</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00085</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_08-36-31</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76043    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             644.743</td><td style=\"text-align: right;\">           316.864</td><td style=\"text-align: right;\">       644.743</td><td style=\"text-align: right;\"> 1674387391</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00085</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00086</td><td style=\"text-align: right;\">  0.442076</td><td>2023-01-22_08-41-59</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  5.69013    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.751</td><td style=\"text-align: right;\">           327.751</td><td style=\"text-align: right;\">       327.751</td><td style=\"text-align: right;\"> 1674387719</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00086</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00087</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_08-52-46</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.77181    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             647.157</td><td style=\"text-align: right;\">           317.856</td><td style=\"text-align: right;\">       647.157</td><td style=\"text-align: right;\"> 1674388366</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00087</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00088</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_09-03-27</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76483    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             641.205</td><td style=\"text-align: right;\">           315.044</td><td style=\"text-align: right;\">       641.205</td><td style=\"text-align: right;\"> 1674389007</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00088</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00089</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_09-14-07</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.75931    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             639.944</td><td style=\"text-align: right;\">           315.117</td><td style=\"text-align: right;\">       639.944</td><td style=\"text-align: right;\"> 1674389647</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00089</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00090</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_09-39-41</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1533.85 </td><td style=\"text-align: right;\">           303.318</td><td style=\"text-align: right;\">      1533.85 </td><td style=\"text-align: right;\"> 1674391181</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00090</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00091</td><td style=\"text-align: right;\">  0.720111</td><td>2023-01-22_10-06-18</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  3.57014    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1596.86 </td><td style=\"text-align: right;\">           317.484</td><td style=\"text-align: right;\">      1596.86 </td><td style=\"text-align: right;\"> 1674392778</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00091</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00092</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_10-11-35</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.74466    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             317.496</td><td style=\"text-align: right;\">           317.496</td><td style=\"text-align: right;\">       317.496</td><td style=\"text-align: right;\"> 1674393095</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00092</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00093</td><td style=\"text-align: right;\">  0.520853</td><td>2023-01-22_10-16-54</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.77653    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             319.022</td><td style=\"text-align: right;\">           319.022</td><td style=\"text-align: right;\">       319.022</td><td style=\"text-align: right;\"> 1674393414</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00093</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00094</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_10-27-36</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76715    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             641.186</td><td style=\"text-align: right;\">           314.895</td><td style=\"text-align: right;\">       641.186</td><td style=\"text-align: right;\"> 1674394056</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00094</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00095</td><td style=\"text-align: right;\">  0.435589</td><td>2023-01-22_10-33-02</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  9.06037    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.379</td><td style=\"text-align: right;\">           326.379</td><td style=\"text-align: right;\">       326.379</td><td style=\"text-align: right;\"> 1674394382</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00095</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00096</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_10-43-43</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76608    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             640.527</td><td style=\"text-align: right;\">           314.318</td><td style=\"text-align: right;\">       640.527</td><td style=\"text-align: right;\"> 1674395023</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00096</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00097</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_10-54-29</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76436    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             646.358</td><td style=\"text-align: right;\">           316.665</td><td style=\"text-align: right;\">       646.358</td><td style=\"text-align: right;\"> 1674395669</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00097</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00098</td><td style=\"text-align: right;\">  0.777572</td><td>2023-01-22_11-15-40</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">  2.57574    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1270.96 </td><td style=\"text-align: right;\">           316.589</td><td style=\"text-align: right;\">      1270.96 </td><td style=\"text-align: right;\"> 1674396940</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b426b_00098</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00099</td><td style=\"text-align: right;\">  0.538462</td><td>2023-01-22_11-21-08</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.91287    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.105</td><td style=\"text-align: right;\">           328.105</td><td style=\"text-align: right;\">       328.105</td><td style=\"text-align: right;\"> 1674397268</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00099</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00100</td><td style=\"text-align: right;\">  0.559778</td><td>2023-01-22_11-26-27</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.73321    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             318.226</td><td style=\"text-align: right;\">           318.226</td><td style=\"text-align: right;\">       318.226</td><td style=\"text-align: right;\"> 1674397587</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00100</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00101</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_11-52-06</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1539.18 </td><td style=\"text-align: right;\">           304.869</td><td style=\"text-align: right;\">      1539.18 </td><td style=\"text-align: right;\"> 1674399126</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00101</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00102</td><td style=\"text-align: right;\">  0.565338</td><td>2023-01-22_11-57-32</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 19.1544     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             326.265</td><td style=\"text-align: right;\">           326.265</td><td style=\"text-align: right;\">       326.265</td><td style=\"text-align: right;\"> 1674399452</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00102</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00103</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_12-08-23</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76561    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             650.796</td><td style=\"text-align: right;\">           318.998</td><td style=\"text-align: right;\">       650.796</td><td style=\"text-align: right;\"> 1674400103</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00103</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00104</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_12-34-10</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1546.99 </td><td style=\"text-align: right;\">           305.977</td><td style=\"text-align: right;\">      1546.99 </td><td style=\"text-align: right;\"> 1674401650</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00104</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00105</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_12-59-57</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">nan          </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1546.64 </td><td style=\"text-align: right;\">           304.901</td><td style=\"text-align: right;\">      1546.64 </td><td style=\"text-align: right;\"> 1674403197</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00105</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00106</td><td style=\"text-align: right;\">  0.756256</td><td>2023-01-22_13-05-26</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 25.1868     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             328.943</td><td style=\"text-align: right;\">           328.943</td><td style=\"text-align: right;\">       328.943</td><td style=\"text-align: right;\"> 1674403526</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00106</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00107</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_13-16-17</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.76869    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             651.233</td><td style=\"text-align: right;\">           320.447</td><td style=\"text-align: right;\">       651.233</td><td style=\"text-align: right;\"> 1674404177</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00107</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00108</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_13-21-47</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  4.60248    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             330.162</td><td style=\"text-align: right;\">           330.162</td><td style=\"text-align: right;\">       330.162</td><td style=\"text-align: right;\"> 1674404507</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00108</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00109</td><td style=\"text-align: right;\">  0.614458</td><td>2023-01-22_13-27-15</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.78147    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             327.723</td><td style=\"text-align: right;\">           327.723</td><td style=\"text-align: right;\">       327.723</td><td style=\"text-align: right;\"> 1674404835</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00109</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00110</td><td style=\"text-align: right;\">  0.447637</td><td>2023-01-22_13-32-48</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  4.12129    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             332.952</td><td style=\"text-align: right;\">           332.952</td><td style=\"text-align: right;\">       332.952</td><td style=\"text-align: right;\"> 1674405168</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00110</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00111</td><td style=\"text-align: right;\">  0.706209</td><td>2023-01-22_13-38-18</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 10.7771     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             329.982</td><td style=\"text-align: right;\">           329.982</td><td style=\"text-align: right;\">       329.982</td><td style=\"text-align: right;\"> 1674405498</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00111</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00112</td><td style=\"text-align: right;\">  0.531974</td><td>2023-01-22_13-43-48</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.40758    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             330.172</td><td style=\"text-align: right;\">           330.172</td><td style=\"text-align: right;\">       330.172</td><td style=\"text-align: right;\"> 1674405828</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00112</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00113</td><td style=\"text-align: right;\">  0.62836 </td><td>2023-01-22_13-49-20</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  3.63801    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             331.27 </td><td style=\"text-align: right;\">           331.27 </td><td style=\"text-align: right;\">       331.27 </td><td style=\"text-align: right;\"> 1674406160</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00113</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00114</td><td style=\"text-align: right;\">  0.542169</td><td>2023-01-22_13-54-41</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.74768    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             320.83 </td><td style=\"text-align: right;\">           320.83 </td><td style=\"text-align: right;\">       320.83 </td><td style=\"text-align: right;\"> 1674406481</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00114</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00115</td><td style=\"text-align: right;\">  0.511585</td><td>2023-01-22_14-00-03</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">  2.7628     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             322.325</td><td style=\"text-align: right;\">           322.325</td><td style=\"text-align: right;\">       322.325</td><td style=\"text-align: right;\"> 1674406803</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00115</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00116</td><td style=\"text-align: right;\">  0.564411</td><td>2023-01-22_14-10-57</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">  2.75337    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             653.732</td><td style=\"text-align: right;\">           321.464</td><td style=\"text-align: right;\">       653.732</td><td style=\"text-align: right;\"> 1674407457</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00116</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00117</td><td style=\"text-align: right;\">  0.434662</td><td>2023-01-22_14-21-49</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 50.0007     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             652.407</td><td style=\"text-align: right;\">           321.521</td><td style=\"text-align: right;\">       652.407</td><td style=\"text-align: right;\"> 1674408109</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b426b_00117</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00118</td><td style=\"text-align: right;\">  0.555144</td><td>2023-01-22_14-27-07</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 13.0908     </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">             317.591</td><td style=\"text-align: right;\">           317.591</td><td style=\"text-align: right;\">       317.591</td><td style=\"text-align: right;\"> 1674408427</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>b426b_00118</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "<tr><td>objective_b426b_00119</td><td style=\"text-align: right;\">  0.783133</td><td>2023-01-22_14-53-46</td><td>True  </td><td>                </td><td>69e6eed4af5c47e1a0e81902469decef</td><td>arch-pc   </td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">  1.93371    </td><td>192.168.1.20</td><td style=\"text-align: right;\">86030</td><td style=\"text-align: right;\">            1599.29 </td><td style=\"text-align: right;\">           317.313</td><td style=\"text-align: right;\">      1599.29 </td><td style=\"text-align: right;\"> 1674410026</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   5</td><td>b426b_00119</td><td style=\"text-align: right;\">   0.00342393</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 8.11541\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 57.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0457\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4429\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0859\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1439\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4672.67300\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.18 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74018\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.54784\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.73683\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.53746\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.73857\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.53634\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74037\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4093\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8149\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5449\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.84529\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 44.49 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 7.28927\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 46.25 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4302\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5390\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4263\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3818\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4028\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 52.58 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4707\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4143\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4407\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.54739\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.12 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.17525\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 65.43 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5706\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6776\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4983\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6439\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5618\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 31823884698649301614592.00000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.92 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 190116418056165536.00000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4430\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2970\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4065\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2079\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2751\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.73141\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 55.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.72134\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 58.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5691\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2191\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3164\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5732\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2051\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.49550\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.64940\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 58.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.1284\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7700\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0682\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1253\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.36606\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.59225\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 63.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8103\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3208\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7993\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1904\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3076\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.23261\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.08 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.50789\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.60 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7536\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4489\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5627\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7836\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3977\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5276\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.06299\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 70.17 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.38334\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7592\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6513\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5778\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1973\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2941\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.61867\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.39 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 11.90245\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 68.86 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8988\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4734\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6944\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5827\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6337\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18342\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.52 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.03874\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.59 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7015\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8851\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7827\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5072\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7777\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6140\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 1634.90658\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.04 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76916\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6150\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1988\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3005\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60396\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.25 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.33853\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7827\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6258\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7458\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5562\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6372\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.72133\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.47 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 38.25305\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.61 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7561\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7851\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7704\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5793\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2716\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3698\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62174\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.25 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.28565\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8427\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6106\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7658\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5155\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6162\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.78729\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.10 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.01290\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7328\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7510\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7775\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7396\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7581\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.06142\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 80.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.95093\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7030\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8511\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7700\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7922\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8441\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8173\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.46901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 84.19 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.08208\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.74 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8074\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8430\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8893\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8655\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.16203\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 88.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.71086\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.15 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7080\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8872\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7875\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7145\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5888\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6456\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.16264\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.64 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.94071\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7256\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7705\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5325\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6537\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5869\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 16093.33981\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76802\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6913\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5539\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6150\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.22953\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.91278\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.19 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7846\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7838\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7200\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3667\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4859\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 468.25490\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.50 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76361\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.60669\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75862\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.59624\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75434\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.3239\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0349\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0630\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.72204\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.67 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 8.58652\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 57.92 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.9444\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0697\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.3998\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5080\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.83903\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 44.99 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 6.25698\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 52.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4377\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3706\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4212\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4219\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4215\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.76361\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 52.78 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 23.97916\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.38 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4441\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3425\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4258\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3612\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3908\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.32895\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 54.08 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.68776\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 57.37 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5074\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4495\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5474\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 5958122824913181999104.00000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 52.95 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 14348932641828956.00000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 50.51 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4170\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4233\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6585\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5326\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5889\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.24515\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.96642\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6804\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9106\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7789\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4994\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7573\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6019\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 5.50522\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.03 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76957\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4157\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6252\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4994\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.80337\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 48.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 51.90 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4389\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3745\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4041\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4007\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6859\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5059\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.81524\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 45.36 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 6.09648\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 47.91 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4380\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6915\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5363\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6963\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5948\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6416\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.21300\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.90 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.97345\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.15 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7184\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7818\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4982\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8707\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6338\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 421.35487\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 6.73042\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 42.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4312\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5991\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4301\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3917\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.43943\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 54.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.28694\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 45.97 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4453\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6121\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4429\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2413\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3124\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.74295\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 57.24935\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 55.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4400\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1170\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.1849\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5538\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2109\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3055\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.63392\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 8.30964\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 71.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8774\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3957\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5455\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5411\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1700\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2587\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.64196\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.39451\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.77 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8169\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5128\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6301\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7521\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5483\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6342\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.76319\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.51 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.82547\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8061\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7332\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8028\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7174\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7577\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.79796\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 80.78 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.69608\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 83.69 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8341\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8066\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8784\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8511\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.10717\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 87.90 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 4.20327\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5874\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7324\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5518\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3961\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4611\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.64600\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.25 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.30741\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 50.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4656\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6341\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5802\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4719\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5205\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.59470\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.54 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 9.18313\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 44.58 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4401\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6112\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6067\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2200\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3229\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60332\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.38 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.29773\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.18 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8318\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5681\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6751\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7398\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4810\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5829\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.97804\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.17 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.18651\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6923\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7277\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7095\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6625\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6945\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.55933\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 75.58 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.98160\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.29 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7273\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7461\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7592\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7290\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7438\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.11858\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 78.96 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.89131\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.41 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8128\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7663\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7990\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8131\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8060\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.58671\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.60 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.19792\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.55 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6709\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9064\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7710\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5333\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2549\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3450\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60670\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.52 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.24691\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7806\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6511\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7100\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7277\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4331\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5430\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.09252\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.38409\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 66.91 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5827\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6904\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6223\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6829\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6512\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.92325\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.05230\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.02 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7284\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7406\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7198\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6918\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7055\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.51503\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 75.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.05360\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.70 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6762\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7347\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7626\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7480\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.13313\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 78.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.68863\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8037\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7489\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7753\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6636\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4431\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5314\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.37637\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.13 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.07294\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.61 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7454\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6570\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4648\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6996\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5690\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6276\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18582\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.82343\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7247\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8681\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7899\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6453\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4092\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5008\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 12389.10394\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76242\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.60429\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75769\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.59431\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75360\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5801\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3187\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4114\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60859\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.22149\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.37 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8349\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6776\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7231\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5111\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5989\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.83823\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.09584\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7762\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7326\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7785\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7006\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7375\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.17366\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 79.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.77959\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8059\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8386\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7963\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8169\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.41451\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 85.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.86531\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8048\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7927\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8664\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8388\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8524\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.10174\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 87.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.71645\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6332\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7568\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7120\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1988\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3108\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.59676\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.33157\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6732\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5421\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6242\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.90133\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.65 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.97338\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.55 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7333\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7294\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7282\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6670\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6963\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.37883\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 75.62 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.02765\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8123\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6170\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7013\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8082\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6794\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7382\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.99984\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 79.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.72849\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7890\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8277\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8079\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8359\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8299\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8329\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.32202\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 86.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.88262\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.20 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6197\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7458\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5858\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4925\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.58314\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.29 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 12.00737\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 44.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4383\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6091\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4379\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1017\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1650\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.70289\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.04 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.61748\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 60.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2544\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6525\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0815\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1449\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.70 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4193\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3429\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3773\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.25184\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 53.84 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 5.00105\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 49.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4317\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5106\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4678\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0835\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1449\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.68965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.84 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 32.41521\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 62.19 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7870\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2941\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4070\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4157\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.77878\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 51.30 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 7.68098\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.75 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4181\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2287\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5801\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.57269\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.43653\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 64.13 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5508\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6993\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4491\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4861\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 6.49334\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74045\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.54820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.73684\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.53741\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.73863\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6630\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4628\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5451\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.35886\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.50 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.89512\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6833\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7664\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3667\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4770\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4562\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3718\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4097\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.33966\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.38524\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 55.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4937\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9128\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6408\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4783\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0668\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1172\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.69377\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.97 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.63026\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 59.41 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7667\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.1736\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6325\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1860\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2875\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.34774\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.37 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.30059\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.07 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7209\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7092\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6966\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6041\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6471\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.83425\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.39 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.21295\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.86 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8730\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4681\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6094\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7738\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5695\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6561\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.50139\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 74.99 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.28732\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.98 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.9065\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4745\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6229\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8112\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6050\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6930\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.31577\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 77.55 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.06934\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.87 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8735\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7128\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4427\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3748\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4059\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.37111\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 55.26 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.89869\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4564\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3733\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4107\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.28596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.05635\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 60.24 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5269\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6515\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4004\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3141\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3520\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.76338\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 52.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.77070\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 52.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4479\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4387\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4398\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3490\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3892\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.37264\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 55.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.89042\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4599\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3841\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5147\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0531\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0963\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.67115\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.34 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.62998\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 63.30 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7126\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2638\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3851\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6293\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1940\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.41514\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.62476\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 61.54 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5346\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6719\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5727\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6103\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5909\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.21056\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.60 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.14915\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7412\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7039\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7028\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5633\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6254\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.82946\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.73 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.06986\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8154\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5638\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6667\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7627\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6120\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6791\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.36822\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 75.77 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.87648\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.43 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8444\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7325\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5708\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1836\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2778\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62948\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.08 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.29164\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8376\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6709\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7143\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2923\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4148\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.23242\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.69875\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7195\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1568\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2575\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.30284\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.12 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.00914\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.63 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6716\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7549\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7142\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7679\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7401\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.25494\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 77.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.57566\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 83.78 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8678\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7991\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8598\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7662\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8103\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.39241\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 84.97 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.49808\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 85.17 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8555\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6970\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4294\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5315\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.38470\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.12 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.01725\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.65 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7995\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6191\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6978\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7962\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2595\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3914\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.20 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6900\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5539\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6145\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.23986\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.66 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.62589\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 83.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7984\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8117\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7698\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3525\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4836\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.85130\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75521\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5590\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4097\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4729\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.55082\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.75 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.77224\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 59.96 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5220\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6757\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5581\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0364\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0684\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.69456\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.67580\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 60.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7264\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1638\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2674\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6284\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2427\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3502\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.36122\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.26 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.29370\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 71.18 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8118\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5710\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7196\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5182\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6025\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.82621\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.12395\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.77 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7101\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6907\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6829\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4464\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5399\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.13 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4463\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3657\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4020\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.56137\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 55.63 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 4.49620\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 62.93 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6316\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4565\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6913\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4689\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5588\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.33882\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.80 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.33813\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 71.92 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6198\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9191\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6182\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3844\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4741\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6821\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2018\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3115\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.58786\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.61 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 8.97619\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8608\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6829\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4771\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4112\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4417\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.24039\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.61 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.99848\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 60.24 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5265\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6549\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7154\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5797\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.15180\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.85530\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.15 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7199\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6516\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4074\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5014\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 14226.33668\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75432\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.63 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0064\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0127\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.3478\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0364\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0659\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.69179\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.92 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 21.33830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 60.06 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7241\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.1340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2262\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4522\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5090\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.52733\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.35923\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 68.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5981\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6929\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5895\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4938\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.57508\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.57272\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 64.87 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5619\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6855\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5105\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5164\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5134\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 604.48133\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.00 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74651\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5941\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4552\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5155\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.52821\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.10 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.32426\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.96 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6740\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7226\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5457\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5288\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5371\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 76307048.63836\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.89074\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7311\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2352\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3559\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.53574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.28 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.15473\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.65 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7766\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6511\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7083\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7180\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5412\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6172\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.83700\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.87 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.87661\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.43 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7696\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7613\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7766\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7112\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7425\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.13932\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 79.33 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.84021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.28 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8160\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7740\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8314\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7555\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7916\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.65753\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.34 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.98044\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7224\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8915\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7981\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8188\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8645\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8410\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.32275\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 86.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.54394\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6245\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9447\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7519\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6034\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1639\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2578\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60123\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.51 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.25414\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.96 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8566\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6177\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7618\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4845\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5923\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.76384\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.06 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.88078\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7927\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6915\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7386\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7978\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6988\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7450\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.89554\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 79.96 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.77049\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7921\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8106\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8013\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8521\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8264\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8390\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.23442\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 86.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.52361\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.57 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6215\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7504\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5755\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2428\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3415\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.36174\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7863\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6106\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6874\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7191\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4980\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 5.14271\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.51998\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 65.80 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5750\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6772\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6248\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6785\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6505\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.85944\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.37342\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 66.73 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5712\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7126\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6672\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8060\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7300\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.30993\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 75.03 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.29120\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6257\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9426\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7521\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4286\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0501\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0897\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.71009\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.54 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 19.93672\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 54.49 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.3820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.1216\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5764\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2003\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2973\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.63370\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.39 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.38683\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 71.55 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8327\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5706\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7171\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4198\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5296\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.03476\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.13730\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7052\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6828\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7216\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6404\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6786\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.49319\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 74.58 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.07498\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.25 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6656\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8681\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7535\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7268\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7564\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7413\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.21180\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 77.88 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.86937\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.50 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7172\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7721\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7847\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8264\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8050\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.58017\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.23 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.17366\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8856\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6426\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7448\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6996\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5584\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6211\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.16431\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.69153\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.37 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7642\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8277\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7947\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5090\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8007\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6224\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 1048.99474\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.29 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.16015\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6165\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4537\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5227\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.54619\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 4.01249\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4124\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7754\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5385\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.81917\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 45.79 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.80515\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 46.06 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4280\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7085\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5337\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7007\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1563\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2556\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62055\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.87 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.96804\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8987\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5854\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5883\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5348\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.52245\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 27.26051\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0042\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6552\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4643\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5435\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.36999\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.19 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.58525\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5956\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7235\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5946\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3871\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4689\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6049\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2231\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3259\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.60463\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.38 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 12.27599\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.88 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5766\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6667\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5778\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1578\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2479\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62112\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.95 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.61208\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 71.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8458\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4319\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5718\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6998\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2994\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4194\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.35846\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.61177\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 66.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8011\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4365\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6976\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4066\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5137\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.14571\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.40477\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6155\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6933\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6664\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6811\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6737\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.81537\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.36 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.44306\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 65.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5716\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6887\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6857\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6201\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.19976\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.81306\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.65 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8238\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7775\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5025\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8078\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6196\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 756250.72828\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.77055\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4347\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3536\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3900\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.20517\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 54.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.40449\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 57.74 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5084\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6508\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5940\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4795\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5306\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.55968\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.41 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.99003\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.16 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7164\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6034\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6621\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4370\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5265\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.38493\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.95 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.01840\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6750\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7554\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6678\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4675\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6074\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2489\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3531\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.59352\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.27038\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4702\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6014\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7049\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5058\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5890\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.88585\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 70.43 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.05386\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 74.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7185\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6980\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7495\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6918\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7195\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.23165\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 77.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.98780\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.59 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7108\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7772\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7864\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8379\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8113\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.66436\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.67 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.21717\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6959\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8766\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7759\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8024\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8919\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8448\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.35500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 86.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.24620\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6053\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9298\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7332\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5571\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4294\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4850\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.65665\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.81 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.52228\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.75 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6423\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6150\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3127\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4146\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 30.73533\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.01 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74014\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4922\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2398\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3224\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.64054\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.91 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.23980\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7839\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6811\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7506\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5438\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6307\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.78623\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.03650\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.48 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7455\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7675\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7077\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7364\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.18002\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 78.78 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.90381\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.70 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7446\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8128\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7772\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8154\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8025\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.61790\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.71 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.91926\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.26 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7575\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7802\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8353\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8087\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8218\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.42740\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 85.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.32252\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.50 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7080\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7774\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7145\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5888\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6456\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.19912\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.64 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.00272\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6981\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7730\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5003\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8370\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6262\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 56481.34239\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.14 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.77151\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6881\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4719\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5599\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.30219\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.74 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.81084\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7761\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7745\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7753\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7689\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3242\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4561\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.61 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5930\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4355\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5022\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.56979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.79 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.91728\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 61.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5477\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6957\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6129\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4476\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5064\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.57651\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.23395\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.33 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7839\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7536\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6622\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3959\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4956\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 10729.10897\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.23 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74612\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4171\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3399\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3746\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.46078\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 53.71 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.78829\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 55.79 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4955\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6181\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5714\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4127\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4793\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.59272\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.43 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.89144\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 52.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4777\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6425\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6987\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5630\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6235\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18663\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.28 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.85075\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7840\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7781\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6872\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3756\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4857\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 303.88992\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76296\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7038\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5948\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6447\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.16391\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.11207\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6590\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9085\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7639\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6095\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4438\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5136\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 506603.69369\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.79 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4692\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4204\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.20881\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.18 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 5.69013\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 44.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4384\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6096\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6913\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5812\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6315\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18820\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.34 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.79715\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 80.07 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7365\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8447\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7869\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5376\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6643\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5943\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 55595.89746\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.00 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.77181\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7082\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5781\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6366\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18137\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.08 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.62953\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 84.15 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7972\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8532\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8243\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7375\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3906\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5107\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 441.98067\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.65 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76483\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7038\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6274\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.19952\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.59 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.68731\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.67 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7665\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8132\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7161\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5150\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2752.86016\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.27 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75931\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6674\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4385\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5293\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.38706\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.19 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.90383\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.57 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6959\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8617\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7700\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6898\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3605\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4735\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.42 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0683\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.1202\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.65548\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 59.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.44918\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.34 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8606\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5280\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7297\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4446\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5526\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.92124\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.89293\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.52 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7748\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7606\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7844\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7219\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7518\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.02475\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 80.04 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.75821\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.30 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8378\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7362\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7837\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8562\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7857\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8194\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.39160\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 85.49 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.01491\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 83.41 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8665\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7319\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7935\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.8870\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8131\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8484\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.21827\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 87.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.57014\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 72.01 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6207\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9191\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7410\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4385\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6434\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5215\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.77559\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 51.86 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74466\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5011\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4034\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4310\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4167\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.77804\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 50.80 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.77653\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 52.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4515\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.4660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4586\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6883\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5630\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6194\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.20228\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.78 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.86797\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.89 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7192\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7927\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5584\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6094\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5828\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 195.67026\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76715\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4770\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3778\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4217\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.15435\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.74 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 9.06037\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4356\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6068\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6946\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6237\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.21593\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.15 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.82819\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.70 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7206\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7892\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6183\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4491\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5203\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 1501.92674\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76608\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7190\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5630\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6315\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.18776\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 73.21 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.95255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6949\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8723\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7736\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5583\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5385\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5482\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 8790.56163\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.82 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76436\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6552\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1730\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2737\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.57895\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 62.56 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.25414\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8023\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5872\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6781\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6783\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5155\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5858\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.10849\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.18428\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6406\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8872\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7440\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7182\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7524\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.23311\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 78.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.02536\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.68 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6987\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7858\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7796\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8459\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8114\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.57526\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 83.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.57574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6867\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7790\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4523\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4135\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.47559\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 55.94 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.91287\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4843\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9213\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6349\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4078\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3187\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3578\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.75954\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 53.34 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.73321\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 55.98 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4921\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3319\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3964\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6965\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4492\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5461\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.35064\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 69.55 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.46579\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.16 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6054\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7253\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6048\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3782\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4654\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 63.60 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4622\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3900\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4230\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.20209\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.62 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 19.15439\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 1.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0021\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0042\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6901\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5812\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6310\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.20943\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.28 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.75722\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 82.67 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7992\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.8017\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6672\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4030\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5025\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 205506.31370\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.57 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76561\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6461\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3657\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4671\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.46184\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.97 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.98925\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.46 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7500\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.6894\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7184\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7421\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2905\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4176\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.05 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6697\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4401\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5311\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.35435\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 68.32 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.85227\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 77.85 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7217\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7588\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7197\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3366\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4587\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.72 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 58.11 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: nan\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7009\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2276\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3436\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.55290\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 64.54 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 25.18677\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 75.63 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8462\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5383\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6580\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7252\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5888\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6499\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.15368\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 74.13 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.88680\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 79.80 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7480\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8085\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7771\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5221\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7015\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5986\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 811.40986\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.59 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76869\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4555\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3885\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4193\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.49946\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.13 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 4.60248\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5957\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4674\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.5238\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.58232\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 65.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.78147\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 61.45 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5350\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8787\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6651\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4320\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3566\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3907\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.28891\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 54.64 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 4.12129\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 44.76 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4377\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9426\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5978\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5916\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1715\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2659\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62065\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 61.39 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 10.77714\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.62 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8558\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3915\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5372\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4095\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3399\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3715\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.43255\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 53.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.40758\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 53.20 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4798\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8851\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6223\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.01\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4608\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3748\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4134\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.28809\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 56.62 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 3.63801\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 62.84 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5475\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8468\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6650\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.3731\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2656\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.3103\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.75258\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 51.86 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.74768\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 54.22 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4487\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2234\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.2983\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4239\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.4689\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4452\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.76469\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 52.35 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.76280\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 51.16 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4490\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.5340\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.4879\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: AdamW \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6937\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6422\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.14398\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.83 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.90285\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 81.09 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7969\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7778\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6870\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3596\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4721\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 76.65247\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 66.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.75337\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 56.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.0000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6767\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.5781\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6236\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.23459\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 71.53 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.42899\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.97 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.5984\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9447\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7327\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4783\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8397\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6095\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 60835.70564\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 54.92 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 50.00067\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 43.47 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4346\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.9894\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6039\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: SGD \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 0.0001\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.4576\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.2049\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2830\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.72198\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 57.67 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 13.09080\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 55.51 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.4808\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.2660\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.3425\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m opt_name: Adam \n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m lr: 1e-05\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.5333\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.1578\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.2436\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.62885\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 60.02 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.33533\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 69.88 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.8796\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.3574\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.5083\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7286\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.3472\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.4703\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 4.11737\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 67.24 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.33709\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 70.71 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6337\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.7766\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.6979\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.6619\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.6900\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.6756\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.77429\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 72.24 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.13796\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 73.40 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6597\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8043\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7248\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7209\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.7529\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.7366\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 3.18889\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 77.44 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 2.10987\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 76.00 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.6677\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8936\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7643\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train precision: 0.7655\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train recall: 0.8441\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Train F1: 0.8029\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training loss: 2.70385\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Training accuracy: 82.63 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation loss: 1.93371\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation accuracy: 78.31 (%)\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation precision: 0.7287\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation recall: 0.8000\n",
      "\u001b[2m\u001b[36m(objective pid=86030)\u001b[0m Validation F1: 0.7627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 14:53:46,973\tINFO tune.py:762 -- Total run time: 93207.54 seconds (93207.33 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "        max_t=5,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    ")\n",
    "\n",
    "reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(objective),\n",
    "            resources={\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=N_TRAIN_EXAMPLES,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697ecf17-2139-41a0-b66d-0118ce9c9233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 1e-05, 'optimizer_name': 'AdamW'}\n",
      "Best trial final validation loss: 1.4980830340749687\n",
      "Best trial final validation accuracy: 0.8517145505097312\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(best_result.metrics[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f4ef38-ce3a-4ad1-b637-008808df49a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>lr</th>\n",
       "      <th>opt</th>\n",
       "      <th>training_iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.740372e+00</td>\n",
       "      <td>0.564411</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564411</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.901164e+17</td>\n",
       "      <td>0.539388</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.383336e+00</td>\n",
       "      <td>0.734013</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.190245e+01</td>\n",
       "      <td>0.688601</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2.762800e+00</td>\n",
       "      <td>0.511585</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2.753374e+00</td>\n",
       "      <td>0.564411</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>5.000067e+01</td>\n",
       "      <td>0.434662</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.309080e+01</td>\n",
       "      <td>0.555144</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.933707e+00</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  accuracy       lr    opt  training_iteration\n",
       "0    2.740372e+00  0.564411  0.00100  AdamW                   5\n",
       "1             NaN  0.564411  0.00001    SGD                   5\n",
       "2    1.901164e+17  0.539388  0.01000   Adam                   2\n",
       "3    2.383336e+00  0.734013  0.00010    SGD                   5\n",
       "4    1.190245e+01  0.688601  0.00001   Adam                   1\n",
       "..            ...       ...      ...    ...                 ...\n",
       "115  2.762800e+00  0.511585  0.00001    SGD                   1\n",
       "116  2.753374e+00  0.564411  0.00010  AdamW                   2\n",
       "117  5.000067e+01  0.434662  0.00010   Adam                   2\n",
       "118  1.309080e+01  0.555144  0.00010    SGD                   1\n",
       "119  1.933707e+00  0.783133  0.00001   Adam                   5\n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = results.get_dataframe()\n",
    "resultado.rename(columns={'config/optimizer_name': 'opt', 'config/lr': 'lr'}, inplace = True)\n",
    "df = resultado[['loss', 'accuracy', 'lr', 'opt', 'training_iteration']]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
