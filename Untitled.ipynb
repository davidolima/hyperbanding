{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525aabd6-ffe3-4ed8-a8d7-b785fbb1d297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7cfd7-cbc2-4451-9a39-0ac20631a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from configs import Inputs\n",
    "from train import get_classification_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.augmentations import get_transforms\n",
    "\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")\n",
    "inputs = Inputs(selected_model='efficientnet-b0')\n",
    "model = get_classification_model(inputs.model_name, 2)\n",
    "checkpoint = torch.load('/home/bernardo/github/sex-age-estimation/backup-bia/patch-1/checkpoint-efficientnet-b0-fold-2-max-acc.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670a3f7-326f-45de-b81a-b16ba39eaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2db30-1379-45a0-ab4e-0151c1221acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 3)\n",
    "baseline = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb6bee-9748-443f-8b8f-2d935d6a90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3045f-6492-4d4b-91c4-1bf1704f588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c1d85-307f-4dc0-851e-d5fa0af52128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [100, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a615223-6641-4f28-9853-1bf257e01edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94f151-fd3d-465a-9d91-46b2605aef50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs = GradientShap(model)\n",
    "\n",
    "# We define a distribution of baselines and draw `n_samples` from that\n",
    "# distribution in order to estimate the expectations of gradients across all baselines\n",
    "baseline_dist = torch.randn(10, 3) * 0.001\n",
    "attributions, delta = gs.attribute(input, stdevs=0.09, n_samples=4, baselines=baseline_dist,\n",
    "                                   target=0, return_convergence_delta=True)\n",
    "print('GradientShap Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0db70c-8292-4776-8ba5-a07222d8b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "#from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from configs import Inputs\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from utils.data import FullRadiographSexDataset\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in range(1,31):\n",
    "    filepath = f'/home/bernardo/datasets/pan-radiographs/splits/{i:02d}.txt'\n",
    "    with open(filepath) as f:\n",
    "        for line in f: #ler cada linha do txt\n",
    "            fname = line.strip().split('/')[2] #retirar o \\n\n",
    "            sex = fname.split('-')[10]\n",
    "            age = fname.split('-')[-2][1:]\n",
    "            months = fname.split('-')[-1][1:3] #home/bernardo/datasets/pan-radiographs/1st-set\n",
    "\n",
    "            if fname.split('-')[0] == 'pan': #separar os arquivos pan e panreport\n",
    "                fpath = os.path.join(f'/home/bernardo/datasets/pan-radiographs/1st-set/images/{fname}')\n",
    "            else:\n",
    "                fpath = os.path.join(f'/home/bernardo/datasets/pan-radiographs/2nd-set/images/{fname}')\n",
    "            image = Image.open(fpath)\n",
    "#image_url = \"https://farm1.staticflickr.com/6/9606553_ccc7518589_z.jpg\"\n",
    "\n",
    "image = Image.open(fpath)\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STDV = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(MEAN,STDV)\n",
    "            ])\n",
    "\n",
    "rgb_img = np.float32(image) / 255\n",
    "print(rgb_img) \n",
    "# input_tensor = preprocess_image(rgb_img,\n",
    "#                                 mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_dataset = FullRadiographSexDataset(\n",
    "    root_dir='../data/pan-radiographs/',\n",
    "    fold_nums=[6, 7, 8, 9, 10],\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# model and weights\n",
    "#inputs = Inputs(selected_model='efficientnet-b0')\n",
    "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "# checkpoint = torch.load('/home/bernardo/github/sex-age-estimation/backup-bia/patch-1/checkpoint-efficientnet-b0-fold-2-max-acc.pth.tar')\n",
    "# model.load_state_dict(checkpoint['state_dict'],strict=False)\n",
    "\n",
    "# #model = deeplabv3_resnet50(pretrained=True, progress=False)\n",
    "# model = model.eval()\n",
    "\n",
    "model = model.cuda()\n",
    "# for idx, (image, label) in tqdm(enumerate(val_dataloader)):\n",
    "#     image, label = image.cuda(), label.cuda()\n",
    "\n",
    "# output = model(input_tensor)\n",
    "# print(type(output), output.keys())\n",
    "# odict_keys(['out', 'aux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971488b6-3ad4-4782-ba8a-d7af3a34b596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open(fpath)\n",
    "image = image.convert('RGB')\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64829459-1955-4f2b-a5dd-a917486cb4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model(image)\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790950f-41f1-4340-9cb4-aa3e4c8ef1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.float32(image) / 255\n",
    "input_tensor = preprocess_image(rgb_img,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba6795-378a-4f28-b39a-3a54df9cbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "input_tensor = input_tensor.cuda()\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c06d4b-dffe-4cdd-8e59-de5968b3e44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
